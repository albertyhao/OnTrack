{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import glob\n",
    "from itertools import chain\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentence_parser = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Art</td>\n",
       "      <td>This article is about the academic discipline ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Art_history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Art</td>\n",
       "      <td>If you're seeing this message, it means we're ...</td>\n",
       "      <td>https://www.khanacademy.org/humanities/art-his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Art</td>\n",
       "      <td>If you're seeing this message, it means we're ...</td>\n",
       "      <td>https://www.khanacademy.org/humanities/art-his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Art</td>\n",
       "      <td>Art history, also called art historiography, h...</td>\n",
       "      <td>https://www.britannica.com/art/art-history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Art</td>\n",
       "      <td>\"Why study art history when there are many oth...</td>\n",
       "      <td>https://www.iesa.edu/paris/news-events/art-his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Art</td>\n",
       "      <td>The Definition of Islamic Art\\n\\nThe objective...</td>\n",
       "      <td>https://apcentral.collegeboard.org/courses/ap-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Art</td>\n",
       "      <td>Building a Context\\n\\nArchitecture can be a ch...</td>\n",
       "      <td>https://apcentral.collegeboard.org/courses/ap-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Art</td>\n",
       "      <td>The art of Africa covers a broad region, of co...</td>\n",
       "      <td>http://www.arthistory.net/african-art/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   Art  This article is about the academic discipline ...   \n",
       "1   Art  If you're seeing this message, it means we're ...   \n",
       "2   Art  If you're seeing this message, it means we're ...   \n",
       "3   Art  Art history, also called art historiography, h...   \n",
       "4   Art  \"Why study art history when there are many oth...   \n",
       "5   Art  The Definition of Islamic Art\\n\\nThe objective...   \n",
       "6   Art  Building a Context\\n\\nArchitecture can be a ch...   \n",
       "7   Art  The art of Africa covers a broad region, of co...   \n",
       "\n",
       "                                                 url  \n",
       "0          https://en.wikipedia.org/wiki/Art_history  \n",
       "1  https://www.khanacademy.org/humanities/art-his...  \n",
       "2  https://www.khanacademy.org/humanities/art-his...  \n",
       "3         https://www.britannica.com/art/art-history  \n",
       "4  https://www.iesa.edu/paris/news-events/art-his...  \n",
       "5  https://apcentral.collegeboard.org/courses/ap-...  \n",
       "6  https://apcentral.collegeboard.org/courses/ap-...  \n",
       "7             http://www.arthistory.net/african-art/  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = open(\"articledata.json\")\n",
    "d = json.load(fp)\n",
    "l = []\n",
    "for url in d.keys():\n",
    "    x = {'text': d[url]}\n",
    "    if d[url]:\n",
    "        x['url'] = url\n",
    "        x['label'] = 'Art'\n",
    "        l.append(x)\n",
    "df = pd.DataFrame(l)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for f in df['text']:\n",
    "    words.append(f)\n",
    "\n",
    "words = list(chain.from_iterable(words))\n",
    "words = ''.join(words)[:-1]\n",
    "sentences = words.split('\\n')\n",
    "clean_sentences = [];\n",
    "\n",
    "for s in sentences:\n",
    "    for x in s.split(\".\"):\n",
    "        word_count = len(x.split(\" \"))\n",
    "        if word_count > 5:\n",
    "            clean_sentences.append(x)\n",
    "            \n",
    "sentences = clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 103], ('the', 560), ('of', 469), ('and', 328), ('to', 216)]\n",
      "Sample data [[40, 537, 8, 59, 1, 361, 95, 2, 6, 12], [73, 16, 538, 2, 1, 12, 2, 6, 1030, 362, 49, 2, 6]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 40000\n",
    "\n",
    "def build_dataset(sentences):\n",
    "    words = ''.join(sentences).split()\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    unk_count = 0\n",
    "    sent_data = []\n",
    "    for sentence in sentences:\n",
    "        data = []\n",
    "        for word in sentence.split():\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            data.append(index)\n",
    "        sent_data.append(data)\n",
    "    \n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return sent_data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(sentences)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:2])\n",
    "# del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9712\n"
     ]
    }
   ],
   "source": [
    "skip_window = 3\n",
    "instances = 0\n",
    "\n",
    "# Pad sentence with skip_windows\n",
    "for i in range(len(data)):\n",
    "    data[i] = [vocabulary_size]*skip_window+data[i]+[vocabulary_size]*skip_window\n",
    "\n",
    "# Check how many training samples that we get    \n",
    "for sentence  in data:\n",
    "    instances += len(sentence)-2*skip_window\n",
    "print(instances)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_len = np.array([len(d) for d in data])\n",
    "plt.hist(sent_len,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = np.zeros((instances,skip_window*2+1),dtype=np.int32)\n",
    "labels = np.zeros((instances,1),dtype=np.int32)\n",
    "doc = np.zeros((instances,1),dtype=np.int32)\n",
    "\n",
    "k = 0\n",
    "for doc_id, sentence  in enumerate(data):\n",
    "    for i in range(skip_window, len(sentence)-skip_window):\n",
    "#         buffer = sentence[i-skip_window:i+skip_window+1]\n",
    "#         labels[k] = sentence[i]\n",
    "#         del buffer[skip_window]\n",
    "#         context[k] = buffer\n",
    "#         doc[k] = doc_id\n",
    "#         k += 1\n",
    "        context[k] = sentence[i-skip_window:i+skip_window+1] # Get surrounding words\n",
    "        labels[k] = sentence[i] # Get target variable\n",
    "        doc[k] = doc_id\n",
    "        k += 1\n",
    "        \n",
    "context = np.delete(context,skip_window,1) # delete the middle word        \n",
    "        \n",
    "shuffle_idx = np.random.permutation(k)\n",
    "labels = labels[shuffle_idx]\n",
    "doc = doc[shuffle_idx]\n",
    "context = context[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "context_window = 2*skip_window\n",
    "embedding_size = 50 # Dimension of the embedding vector.\n",
    "softmax_width = embedding_size # +embedding_size2+embedding_size3\n",
    "num_sampled = 5 # Number of negative examples to sample.\n",
    "sum_ids = np.repeat(np.arange(batch_size),context_window)\n",
    "\n",
    "len_docs = len(data)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(): # , tf.device('/cpu:0')\n",
    "    # Input data.\n",
    "    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size*context_window])\n",
    "    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    segment_ids = tf.constant(sum_ids, dtype=tf.int32)\n",
    "\n",
    "    word_embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "    word_embeddings = tf.concat([word_embeddings,tf.zeros((1,embedding_size))],0)\n",
    "    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))\n",
    "\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, softmax_width],\n",
    "                             stddev=1.0 / np.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed_words = tf.segment_mean(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),segment_ids)\n",
    "    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)\n",
    "    embed = (embed_words+embed_docs)/2.0#+embed_hash+embed_users\n",
    "\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(softmax_weights, softmax_biases, train_labels, \n",
    "                                         embed, num_sampled, vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "        \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(doc_embeddings), 1, keep_dims=True))\n",
    "    normalized_doc_embeddings = doc_embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Chunk the data to be passed into the tensorflow Model\n",
    "###########################\n",
    "data_idx = 0\n",
    "def generate_batch(batch_size):\n",
    "    global data_idx\n",
    "\n",
    "    if data_idx+batch_size<instances:\n",
    "        batch_labels = labels[data_idx:data_idx+batch_size]\n",
    "        batch_doc_data = doc[data_idx:data_idx+batch_size]\n",
    "        batch_word_data = context[data_idx:data_idx+batch_size]\n",
    "        data_idx += batch_size\n",
    "    else:\n",
    "        overlay = batch_size - (instances-data_idx)\n",
    "        batch_labels = np.vstack([labels[data_idx:instances],labels[:overlay]])\n",
    "        batch_doc_data = np.vstack([doc[data_idx:instances],doc[:overlay]])\n",
    "        batch_word_data = np.vstack([context[data_idx:instances],context[:overlay]])\n",
    "        data_idx = overlay\n",
    "    batch_word_data = np.reshape(batch_word_data,(-1,1))\n",
    "\n",
    "    return batch_labels, batch_word_data, batch_doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 36.700085\n",
      "Average loss at step 50000: 6.756837\n",
      "Average loss at step 100000: 1.659259\n",
      "Average loss at step 150000: 0.714117\n",
      "Average loss at step 200000: 0.396829\n",
      "Average loss at step 250000: 0.266391\n",
      "Average loss at step 300000: 0.203882\n",
      "Average loss at step 350000: 0.175438\n",
      "Average loss at step 400000: 0.158020\n",
      "Average loss at step 450000: 0.148953\n",
      "Average loss at step 500000: 0.143333\n",
      "Average loss at step 550000: 0.140133\n",
      "Average loss at step 600000: 0.137871\n",
      "Average loss at step 650000: 0.136385\n",
      "Average loss at step 700000: 0.134816\n",
      "Average loss at step 750000: 0.134080\n",
      "Average loss at step 800000: 0.132600\n",
      "Average loss at step 850000: 0.131824\n",
      "Average loss at step 900000: 0.130726\n",
      "Average loss at step 950000: 0.130559\n",
      "Average loss at step 1000000: 0.129895\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000001\n",
    "step_delta = int(num_steps/20)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_labels, batch_word_data, batch_doc_data\\\n",
    "        = generate_batch(batch_size)\n",
    "        feed_dict = {train_word_dataset : np.squeeze(batch_word_data),\n",
    "                     train_doc_dataset : np.squeeze(batch_doc_data),\n",
    "                     train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % step_delta == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / step_delta\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "\n",
    "    # Get the weights to save for later\n",
    "#     final_doc_embeddings = normalized_doc_embeddings.eval()\n",
    "    final_word_embeddings = word_embeddings.eval()\n",
    "    final_word_embeddings_out = softmax_weights.eval()\n",
    "    final_doc_embeddings = normalized_doc_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.805142\n",
      "0.7586118\n",
      "0.74317455\n",
      "0.39424738\n",
      "Arnold Hauser wrote the first Marxist survey of Western Art, entitled The Social History of Art\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rand_doc = np.random.randint(len_docs)\n",
    "dist = final_doc_embeddings.dot(final_doc_embeddings[rand_doc][:,None])\n",
    "closest_doc = np.argsort(dist,axis=0)[-4:][::-1]\n",
    "furthest_doc = np.argsort(dist,axis=0)[0][::-1]\n",
    "\n",
    "for idx in closest_doc:\n",
    "    print(dist[idx][0][0])\n",
    "    \n",
    "print(dist[furthest_doc][0][0])    \n",
    "print(sentences[176])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADPBJREFUeJzt3X2oZHUdx/H3tyeCstq6V1nS6y3YxCUq6yJCUJZUpqA940K1lnUjyh6wwOqPov5oCUqIolhrcYsyenYry2wzlkKjNc3WrDTbalPczLIgetC+/TFHua337pyZOfNwv/t+wWVmzpw75/vjzP3wu2fme05kJpKk9e9B0y5AktQNA12SijDQJakIA12SijDQJakIA12SijDQJakIA12SijDQJamIh0xyY3Nzc7m4uDjJTUrSunfttdfemZnz/dabaKAvLi6yd+/eSW5Skta9iPhdm/U85CJJRRjoklSEgS5JRRjoklSEgS5JRRjoklSEgS5JRRjoklSEgS5JRUy0U1SzZfHCb91/f/+2M6dYiaQuOEOXpCIMdEkqwkCXpCIMdEkqwkCXpCIMdEkqwkCXpCIMdEkqwsYi9TXuBiQbnKRuOEOXpCIMdEkqwkCXpCIMdEkqwkCXpCL6BnpEHBcRV0XETRFxY0S8tVn+2Ii4MiJubm43jL9cSdJa2szQ7wEuyMwTgVOAN0XEZuBCYHdmbgJ2N48lSVPSN9Az8/bM/Glz/+/ATcDjgbOBnc1qO4EXjatISVJ/Ax1Dj4hF4CTgx8AxmXk79EIfOLrr4iRJ7bXuFI2IRwJfAd6WmX+LiLa/twwsAywsLAxTo1qy41I6srWaoUfEQ+mF+ecy86vN4jsiYmPz/Ebg4Gq/m5nbM3MpM5fm5+e7qFmStIo233IJ4NPATZn5kRVP7QK2Nve3Apd1X54kqa02h1yeCbwK+HlEXN8sezewDfhiRJwH/B54+XhKlCS10TfQM/OHwFoHzE/rthxJ0rDsFJWkIgx0SSrCQJekIgx0SSrCS9DpAVY2KElaP5yhS1IRBrokFWGgS1IRBrokFWGgS1IRBrokFWGgS1IRBrokFWFjkTrnlZOk6XCGLklFGOiSVISBLklFGOiSVISBLklFGOiSVISBLklFGOiSVISBLklF2CmqTqx12Tq7RqXJcYYuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJURN9Aj4gdEXEwIvatWPa+iPhjRFzf/Jwx3jIlSf20maFfApy+yvKLMvNpzc/l3ZYlSRpU30DPzD3AXROoRZI0glGOob85Im5oDsls6KwiSdJQhr0E3SeADwDZ3H4YeO1qK0bEMrAMsLCwMOTmNG5rXULucOsNekm5ri5H52XtpNUNNUPPzDsy897M/C9wMXDyYdbdnplLmbk0Pz8/bJ2SpD6GCvSI2Lji4YuBfWutK0majL6HXCLiUuBUYC4iDgDvBU6NiKfRO+SyH3jDGGuUJLXQN9Azc8sqiz89hlokSSOwU1SSijDQJakIA12SijDQJamIYRuL1KE2jTLjaMoZVZevJWl0ztAlqQgDXZKKMNAlqQgDXZKKMNAlqQgDXZKKMNAlqQgDXZKKsLFonbO5R9J9nKFLUhEGuiQVYaBLUhEGuiQVYaBLUhEGuiQVYaBLUhEGuiQVYaBLUhF2imqmrHWpPTtipf6coUtSEQa6JBVhoEtSEQa6JBVhoEtSEQa6JBVhoEtSEQa6JBVhY9EMq9xM02ZslccvjYMzdEkqwkCXpCIMdEkqwkCXpCIMdEkqom+gR8SOiDgYEftWLHtsRFwZETc3txvGW6YkqZ82M/RLgNMPWXYhsDszNwG7m8eSpCnqG+iZuQe465DFZwM7m/s7gRd1XJckaUDDHkM/JjNvB2huj+6uJEnSMMbeKRoRy8AywMLCwrg3d0Swg1LSaoadod8RERsBmtuDa62Ymdszcykzl+bn54fcnCSpn2EDfRewtbm/Fbism3IkScNq87XFS4GrgRMi4kBEnAdsA54XETcDz2seS5KmqO8x9MzcssZTp3VciyRpBHaKSlIRBrokFWGgS1IRBrokFeEl6GZMV01DR2Lz0cox79925hQrkabDGbokFWGgS1IRBrokFWGgS1IRBrokFWGgS1IRBrokFWGgS1IRNhaNgQ0u07fWPnDfqDJn6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJUhJ2iE3QkXhZulq21P+wg1XrlDF2SijDQJakIA12SijDQJakIA12SijDQJakIA12SijDQJakIA12SijDQJakIA12SijDQJakIA12SijDQJamIkU6fGxH7gb8D9wL3ZOZSF0VJkgbXxfnQn5OZd3bwOpKkEXjIRZKKGDXQE/huRFwbEctdFCRJGs6oh1yemZm3RcTRwJUR8cvM3LNyhSbolwEWFhZG3Jz0/9pc1s9L/+lIMdIMPTNva24PAl8DTl5lne2ZuZSZS/Pz86NsTpJ0GEMHekQ8IiKOuu8+8HxgX1eFSZIGM8ohl2OAr0XEfa/z+cz8TidVSZIGNnSgZ+atwFM7rEWSNAK/tihJRRjoklSEgS5JRRjoklREF+dyETavVLVyv+7fduYUK5H6c4YuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUXYKbpCm65AO0LVj92lmhZn6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUUY6JJUhIEuSUUccY1FwzQGjdJMZCPS+tPle8QmI02SM3RJKsJAl6QiDHRJKsJAl6QiDHRJKsJAl6QiDHRJKsJAl6Qi1k1j0VoNGjbuaFLG9V4bR/ORDU3Td+j7ZRL7wRm6JBVhoEtSEQa6JBVhoEtSEQa6JBUxUqBHxOkR8auIuCUiLuyqKEnS4IYO9Ih4MPBx4IXAZmBLRGzuqjBJ0mBGmaGfDNySmbdm5r+BLwBnd1OWJGlQowT644E/rHh8oFkmSZqCyMzhfjHi5cALMvN1zeNXASdn5vmHrLcMLDcPTwB+NXy595sD7uzgdaatyjigzlgcx2ypMg4YbSzHZ+Z8v5VGaf0/ABy34vGxwG2HrpSZ24HtI2znASJib2Yudfma01BlHFBnLI5jtlQZB0xmLKMccvkJsCkinhARDwPOAXZ1U5YkaVBDz9Az856IeDNwBfBgYEdm3thZZZKkgYx0tsXMvBy4vKNaBtHpIZwpqjIOqDMWxzFbqowDJjCWoT8UlSTNFlv/JamImQ70tqcWiIiXRURGxEx+Gt5vHBFxbkT8KSKub35eN406+2mzPyLiFRHxi4i4MSI+P+ka22ixPy5asS9+HRF/nUadbbQYy0JEXBUR10XEDRFxxjTq7KfFOI6PiN3NGH4QEcdOo85+ImJHRByMiH1rPB8R8dFmnDdExNM7LSAzZ/KH3getvwGeCDwM+BmweZX1jgL2ANcAS9Oue5hxAOcCH5t2rR2MYxNwHbCheXz0tOse9n21Yv3z6X3gP/Xah9wn24E3Nvc3A/unXfeQ4/gSsLW5/1zgs9Oue42xPAt4OrBvjefPAL4NBHAK8OMutz/LM/S2pxb4APAh4J+TLG4AVU6R0GYcrwc+npl/AcjMgxOusY1B98cW4NKJVDa4NmNJ4FHN/UezSq/IDGgzjs3A7ub+Vas8PxMycw9w12FWORv4TPZcAzwmIjZ2tf1ZDvS+pxaIiJOA4zLzm5MsbEBtT5Hw0uZfsC9HxHGrPD9tbcbxJOBJEfGjiLgmIk6fWHXttT5lRUQcDzwB+P4E6hpGm7G8D3hlRByg942085k9bcbxM+Clzf0XA0dFxOMmUFvXxnrKlFkO9Fhl2f1fyYmIBwEXARdMrKLhHHYcjW8Ai5n5FOB7wM6xVzW4NuN4CL3DLqfSm9l+KiIeM+a6BtVmHPc5B/hyZt47xnpG0WYsW4BLMvNYev/uf7b525klbcbxDuDZEXEd8Gzgj8A94y5sDAZ5/w1s1nbsSv1OLXAU8GTgBxGxn97xqF0z+MFo31MkZOafM/NfzcOLgWdMqLZBtDnVwwHgssz8T2b+lt55ezZNqL62Wp2yonEOs3u4BdqN5TzgiwCZeTXwcHrnFJklbf5GbsvMl2TmScB7mmV3T67Ezgzy/hvYLAf6YU8tkJl3Z+ZcZi5m5iK9D0XPysy90yl3TX1PkXDIMbSzgJsmWF9bbU718HXgOQARMUfvEMytE62yv1anrIiIE4ANwNUTrm8Qbcbye+A0gIg4kV6g/2miVfbX5m9kbsV/Fu8Cdky4xq7sAl7dfNvlFODuzLy9qxcfqVN0nHKNUwtExPuBvZm5Ls4b03Icb4mIs+j9C3kXvW+9zJSW47gCeH5E/AK4F3hnZv55elU/0ADvqy3AF7L5asIsajmWC4CLI+Lt9P61P3fWxtRyHKcCH4yIpPettjdNreDDiIhL6dU613xu8V7goQCZ+Ul6n2OcAdwC/AN4Tafbn7F9K0ka0iwfcpEkDcBAl6QiDHRJKsJAl6QiDHRJKsJAl6QiDHRJKsJAl6Qi/geSuRx/555fcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dist,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Museum studies, including the history of museum collecting and display, is now a specialized field of study, as is the history of collecting'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[rand_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Museum studies and collecting [ edit ]'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[closest_doc[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Chicago: University of Chicago Press'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[closest_doc[2][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feminist art history [ edit ]'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[closest_doc[3][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' What does the difference in those materials imply? Stone can only span relatively small spaces (the Parthenon had little interior space); steel and concrete allow spatial freedom and larger interior spaces (such as classrooms)'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[furthest_doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-e0d4473586e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnorm_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclosest_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_vec' is not defined"
     ]
    }
   ],
   "source": [
    "norm_vec[rand_doc].dot(norm_vec[closest_doc[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(dist,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpus = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    for i,sentence in enumerate(words.split('\\n')):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(sentence), [i])\n",
    "\n",
    "train_corpus = list(read_corpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=1, dm_concat=0, size=embedding_size, window=skip_window, \n",
    "                negative=5,hs=0, min_count=5, workers=cpus, iter=2)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "closest_doc2 = model.docvecs.most_similar([model.docvecs[rand_doc]],topn=4)\n",
    "for _, sim in closest_doc2:\n",
    "    print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[rand_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[closest_doc2[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[closest_doc2[2][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[closest_doc2[3][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_vec = np.array([vec for vec in model.docvecs])\n",
    "norm_vec = norm_vec/np.sqrt(np.sum(np.square(norm_vec),axis=1,keepdims=True))\n",
    "\n",
    "norm_vec[rand_doc].dot(norm_vec[closest_doc[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
