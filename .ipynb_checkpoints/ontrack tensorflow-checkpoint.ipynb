{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Doc2Vec"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"import collections\nimport glob\nfrom itertools import chain\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\nnp.random.seed(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"files = glob.glob('*.txt')\n\nwords = []\nfor f in files:\n    file = open(f)\n    words.append(file.read())\n    file.close()\n\nwords = list(chain.from_iterable(words))\nwords = ''.join(words)[:-1]\nsentences = words.split('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"vocabulary_size = 40000\n\ndef build_dataset(sentences):\n    words = ''.join(sentences).split()\n    count = [['UNK', -1]]\n    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    \n    unk_count = 0\n    sent_data = []\n    for sentence in sentences:\n        data = []\n        for word in sentence.split():\n            if word in dictionary:\n                index = dictionary[word]\n            else:\n                index = 0  # dictionary['UNK']\n                unk_count = unk_count + 1\n            data.append(index)\n        sent_data.append(data)\n    \n    count[0][1] = unk_count\n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n    return sent_data, count, dictionary, reverse_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(sentences)\nprint('Most common words (+UNK)', count[:5])\nprint('Sample data', data[:2])\n# del words  # Hint to reduce memory.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tensorflow Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"skip_window = 3\ninstances = 0\n\n# Pad sentence with skip_windows\nfor i in range(len(data)):\n    data[i] = [vocabulary_size]*skip_window+data[i]+[vocabulary_size]*skip_window\n\n# Check how many training samples that we get    \nfor sentence  in data:\n    instances += len(sentence)-2*skip_window\nprint(instances)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sent_len = np.array([len(d) for d in data])\nplt.hist(sent_len,100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"context = np.zeros((instances,skip_window*2+1),dtype=np.int32)\nlabels = np.zeros((instances,1),dtype=np.int32)\ndoc = np.zeros((instances,1),dtype=np.int32)\n\nk = 0\nfor doc_id, sentence  in enumerate(data):\n    for i in range(skip_window, len(sentence)-skip_window):\n#         buffer = sentence[i-skip_window:i+skip_window+1]\n#         labels[k] = sentence[i]\n#         del buffer[skip_window]\n#         context[k] = buffer\n#         doc[k] = doc_id\n#         k += 1\n        context[k] = sentence[i-skip_window:i+skip_window+1] # Get surrounding words\n        labels[k] = sentence[i] # Get target variable\n        doc[k] = doc_id\n        k += 1\n        \ncontext = np.delete(context,skip_window,1) # delete the middle word        \n        \nshuffle_idx = np.random.permutation(k)\nlabels = labels[shuffle_idx]\ndoc = doc[shuffle_idx]\ncontext = context[shuffle_idx]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"batch_size = 256\ncontext_window = 2*skip_window\nembedding_size = 50 # Dimension of the embedding vector.\nsoftmax_width = embedding_size # +embedding_size2+embedding_size3\nnum_sampled = 5 # Number of negative examples to sample.\nsum_ids = np.repeat(np.arange(batch_size),context_window)\n\nlen_docs = len(data)\n\ngraph = tf.Graph()\n\nwith graph.as_default(): # , tf.device('/cpu:0')\n    # Input data.\n    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size*context_window])\n    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n\n    segment_ids = tf.constant(sum_ids, dtype=tf.int32)\n\n    word_embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n    word_embeddings = tf.concat([word_embeddings,tf.zeros((1,embedding_size))],0)\n    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))\n\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, softmax_width],\n                             stddev=1.0 / np.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Model.\n    # Look up embeddings for inputs.\n    embed_words = tf.segment_mean(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),segment_ids)\n    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)\n    embed = (embed_words+embed_docs)/2.0#+embed_hash+embed_users\n\n    # Compute the softmax loss, using a sample of the negative labels each time.\n    loss = tf.reduce_mean(tf.nn.nce_loss(softmax_weights, softmax_biases, train_labels, \n                                         embed, num_sampled, vocabulary_size))\n\n    # Optimizer.\n    optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n        \n    norm = tf.sqrt(tf.reduce_sum(tf.square(doc_embeddings), 1, keep_dims=True))\n    normalized_doc_embeddings = doc_embeddings / norm","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"############################\n# Chunk the data to be passed into the tensorflow Model\n###########################\ndata_idx = 0\ndef generate_batch(batch_size):\n    global data_idx\n\n    if data_idx+batch_size<instances:\n        batch_labels = labels[data_idx:data_idx+batch_size]\n        batch_doc_data = doc[data_idx:data_idx+batch_size]\n        batch_word_data = context[data_idx:data_idx+batch_size]\n        data_idx += batch_size\n    else:\n        overlay = batch_size - (instances-data_idx)\n        batch_labels = np.vstack([labels[data_idx:instances],labels[:overlay]])\n        batch_doc_data = np.vstack([doc[data_idx:instances],doc[:overlay]])\n        batch_word_data = np.vstack([context[data_idx:instances],context[:overlay]])\n        data_idx = overlay\n    batch_word_data = np.reshape(batch_word_data,(-1,1))\n\n    return batch_labels, batch_word_data, batch_doc_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"num_steps = 1000001\nstep_delta = int(num_steps/20)\n\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    average_loss = 0\n    for step in range(num_steps):\n        batch_labels, batch_word_data, batch_doc_data\\\n        = generate_batch(batch_size)\n        feed_dict = {train_word_dataset : np.squeeze(batch_word_data),\n                     train_doc_dataset : np.squeeze(batch_doc_data),\n                     train_labels : batch_labels}\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += l\n        if step % step_delta == 0:\n            if step > 0:\n                average_loss = average_loss / step_delta\n            # The average loss is an estimate of the loss over the last 2000 batches.\n            print('Average loss at step %d: %f' % (step, average_loss))\n            average_loss = 0\n\n    # Get the weights to save for later\n#     final_doc_embeddings = normalized_doc_embeddings.eval()\n    final_word_embeddings = word_embeddings.eval()\n    final_word_embeddings_out = softmax_weights.eval()\n    final_doc_embeddings = normalized_doc_embeddings.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# rand_doc = np.random.randint(len_docs)\ndist = final_doc_embeddings.dot(final_doc_embeddings[rand_doc][:,None])\nclosest_doc = np.argsort(dist,axis=0)[-4:][::-1]\nfurthest_doc = np.argsort(dist,axis=0)[0][::-1]\n\nfor idx in closest_doc:\n    print(dist[idx][0][0])\n    \nprint(dist[furthest_doc][0][0])    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.hist(dist,100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences[rand_doc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences[closest_doc[1][0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences[closest_doc[2][0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences[closest_doc[3][0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences[furthest_doc[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.hist(dist,100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gensim Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"import gensim\nfrom gensim.models import Doc2Vec\nfrom multiprocessing import cpu_count\n\ncpus = cpu_count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def read_corpus():\n    for i,sentence in enumerate(words.split('\\n')):\n        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(sentence), [i])\n\ntrain_corpus = list(read_corpus())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"model = Doc2Vec(dm=1, dm_concat=0, size=embedding_size, window=skip_window, \n                negative=5,hs=0, min_count=5, workers=cpus, iter=2)\nmodel.build_vocab(train_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nmodel.train(train_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"closest_doc2 = model.docvecs.most_similar([model.docvecs[rand_doc]],topn=4)\nfor _, sim in closest_doc2:\n    print(sim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences[rand_doc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences[closest_doc2[1][0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences[closest_doc2[2][0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sentences[closest_doc2[3][0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_vec = np.array([vec for vec in model.docvecs])\nnorm_vec = norm_vec/np.sqrt(np.sum(np.square(norm_vec),axis=1,keepdims=True))\n\nnorm_vec[rand_doc].dot(norm_vec[closest_doc2[1][0]])","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"latex_envs":{"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":0}},"nbformat":4,"nbformat_minor":1}