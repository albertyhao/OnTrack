{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import glob\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('*.txt')\n",
    "\n",
    "words = []\n",
    "for f in files:\n",
    "    file = open(f)\n",
    "    words.append(file.read())\n",
    "    file.close()\n",
    "\n",
    "words = list(chain.from_iterable(words))\n",
    "words = ''.join(words)[:-1]\n",
    "sentences = words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 40000\n",
    "\n",
    "def build_dataset(sentences):\n",
    "    words = ''.join(sentences).split()\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    unk_count = 0\n",
    "    sent_data = []\n",
    "    for sentence in sentences:\n",
    "        data = []\n",
    "        for word in sentence.split():\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            data.append(index)\n",
    "        sent_data.append(data)\n",
    "    \n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return sent_data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(sentences)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:2])\n",
    "# del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_window = 3\n",
    "instances = 0\n",
    "\n",
    "# Pad sentence with skip_windows\n",
    "for i in range(len(data)):\n",
    "    data[i] = [vocabulary_size]*skip_window+data[i]+[vocabulary_size]*skip_window\n",
    "\n",
    "# Check how many training samples that we get    \n",
    "for sentence  in data:\n",
    "    instances += len(sentence)-2*skip_window\n",
    "print(instances)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = np.array([len(d) for d in data])\n",
    "plt.hist(sent_len,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = np.zeros((instances,skip_window*2+1),dtype=np.int32)\n",
    "labels = np.zeros((instances,1),dtype=np.int32)\n",
    "doc = np.zeros((instances,1),dtype=np.int32)\n",
    "\n",
    "k = 0\n",
    "for doc_id, sentence  in enumerate(data):\n",
    "    for i in range(skip_window, len(sentence)-skip_window):\n",
    "#         buffer = sentence[i-skip_window:i+skip_window+1]\n",
    "#         labels[k] = sentence[i]\n",
    "#         del buffer[skip_window]\n",
    "#         context[k] = buffer\n",
    "#         doc[k] = doc_id\n",
    "#         k += 1\n",
    "        context[k] = sentence[i-skip_window:i+skip_window+1] # Get surrounding words\n",
    "        labels[k] = sentence[i] # Get target variable\n",
    "        doc[k] = doc_id\n",
    "        k += 1\n",
    "        \n",
    "context = np.delete(context,skip_window,1) # delete the middle word        \n",
    "        \n",
    "shuffle_idx = np.random.permutation(k)\n",
    "labels = labels[shuffle_idx]\n",
    "doc = doc[shuffle_idx]\n",
    "context = context[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "context_window = 2*skip_window\n",
    "embedding_size = 50 # Dimension of the embedding vector.\n",
    "softmax_width = embedding_size # +embedding_size2+embedding_size3\n",
    "num_sampled = 5 # Number of negative examples to sample.\n",
    "sum_ids = np.repeat(np.arange(batch_size),context_window)\n",
    "\n",
    "len_docs = len(data)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(): # , tf.device('/cpu:0')\n",
    "    # Input data.\n",
    "    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size*context_window])\n",
    "    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    segment_ids = tf.constant(sum_ids, dtype=tf.int32)\n",
    "\n",
    "    word_embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "    word_embeddings = tf.concat([word_embeddings,tf.zeros((1,embedding_size))],0)\n",
    "    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))\n",
    "\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, softmax_width],\n",
    "                             stddev=1.0 / np.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed_words = tf.segment_mean(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),segment_ids)\n",
    "    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)\n",
    "    embed = (embed_words+embed_docs)/2.0#+embed_hash+embed_users\n",
    "\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(softmax_weights, softmax_biases, train_labels, \n",
    "                                         embed, num_sampled, vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "        \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(doc_embeddings), 1, keep_dims=True))\n",
    "    normalized_doc_embeddings = doc_embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Chunk the data to be passed into the tensorflow Model\n",
    "###########################\n",
    "data_idx = 0\n",
    "def generate_batch(batch_size):\n",
    "    global data_idx\n",
    "\n",
    "    if data_idx+batch_size<instances:\n",
    "        batch_labels = labels[data_idx:data_idx+batch_size]\n",
    "        batch_doc_data = doc[data_idx:data_idx+batch_size]\n",
    "        batch_word_data = context[data_idx:data_idx+batch_size]\n",
    "        data_idx += batch_size\n",
    "    else:\n",
    "        overlay = batch_size - (instances-data_idx)\n",
    "        batch_labels = np.vstack([labels[data_idx:instances],labels[:overlay]])\n",
    "        batch_doc_data = np.vstack([doc[data_idx:instances],doc[:overlay]])\n",
    "        batch_word_data = np.vstack([context[data_idx:instances],context[:overlay]])\n",
    "        data_idx = overlay\n",
    "    batch_word_data = np.reshape(batch_word_data,(-1,1))\n",
    "\n",
    "    return batch_labels, batch_word_data, batch_doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000001\n",
    "step_delta = int(num_steps/20)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_labels, batch_word_data, batch_doc_data\\\n",
    "        = generate_batch(batch_size)\n",
    "        feed_dict = {train_word_dataset : np.squeeze(batch_word_data),\n",
    "                     train_doc_dataset : np.squeeze(batch_doc_data),\n",
    "                     train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % step_delta == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / step_delta\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "\n",
    "    # Get the weights to save for later\n",
    "#     final_doc_embeddings = normalized_doc_embeddings.eval()\n",
    "    final_word_embeddings = word_embeddings.eval()\n",
    "    final_word_embeddings_out = softmax_weights.eval()\n",
    "    final_doc_embeddings = normalized_doc_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_doc = np.random.randint(len_docs)\n",
    "dist = final_doc_embeddings.dot(final_doc_embeddings[rand_doc][:,None])\n",
    "closest_doc = np.argsort(dist,axis=0)[-4:][::-1]\n",
    "furthest_doc = np.argsort(dist,axis=0)[0][::-1]\n",
    "\n",
    "for idx in closest_doc:\n",
    "    print(dist[idx][0][0])\n",
    "    \n",
    "print(dist[furthest_doc][0][0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dist,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[rand_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[closest_doc[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[closest_doc[2][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[closest_doc[3][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[furthest_doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(dist,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpus = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    for i,sentence in enumerate(words.split('\\n')):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(sentence), [i])\n",
    "\n",
    "train_corpus = list(read_corpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=1, dm_concat=0, size=embedding_size, window=skip_window, \n",
    "                negative=5,hs=0, min_count=5, workers=cpus, iter=2)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_doc2 = model.docvecs.most_similar([model.docvecs[rand_doc]],topn=4)\n",
    "for _, sim in closest_doc2:\n",
    "    print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[rand_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[closest_doc2[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[closest_doc2[2][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[closest_doc2[3][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vec = np.array([vec for vec in model.docvecs])\n",
    "norm_vec = norm_vec/np.sqrt(np.sum(np.square(norm_vec),axis=1,keepdims=True))\n",
    "\n",
    "norm_vec[rand_doc].dot(norm_vec[closest_doc2[1][0]])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
